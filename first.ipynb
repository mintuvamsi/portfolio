{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the labyrinth of logic's embrace,\\nWhere programs dance with elegant grace,\\nA concept profound, mysterious and rare,\\nWe find the essence of recursion there.\\n\\nA word that whispers, echoes from within,\\nLike a spiraling thought that never grows thin,\\nRecursion, a loop, woven subtle and fine,\\nUnraveling secrets, a design so divine.\\n\\nImagine a mirror, reflecting the truth,\\nRepeating steps, from bouncy start to the sleuth,\\nA code that calls itself, bold and unwavering,\\nCreating patterns, enchanted and captivating.\\n\\nWith each recursive leap, like a poet's refrain,\\nThe program explores, it dances through pain,\\nUnraveling mysteries, solving problems unseen,\\nRecursion paints a picture, as if from a dream.\\n\\nA quest to descend, through layers so deep,\\nBuilding upon the stack, the memories we keep,\\nFrom base to summit, the tower we grow,\\nWith each recursion, knowledge we sow.\\n\\nBut beware, oh weary programmer, take heed,\\nLike a tale trapped in an infinite read,\\nWithout a condition, a boundary, a break,\\nRecursion's enchanted spell, a havoc it may make.\\n\\nYet when balanced, controlled, with wisdom so rare,\\nRecursion weaves miracles, beyond compare,\\nThrough fractal algorithms, it paints a masterpiece,\\nExploring the infinite, harmony it can release.\\n\\nSo let us embrace the recursive flow,\\nWith careful steps, like a dancer, we'll go,\\nIn the world of programming, a poet's delight,\\nRecursion, the waltz, that dances through the night.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "client = OpenAI(api_key='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')\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clien' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m assistant \u001b[38;5;241m=\u001b[39m clien\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39massis\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clien' is not defined"
     ]
    }
   ],
   "source": [
    "assistant = clien.beta.assis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 213\u001b[0m\n\u001b[1;32m    210\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a resume analysis for a job posting. Resume: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Job description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Check for match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Using GPT-4 Turbo to analyze the resume against the job description\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    214\u001b[0m   engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# GPT-4 Turbo engine\u001b[39;00m\n\u001b[1;32m    215\u001b[0m   prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    216\u001b[0m   max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Adjust token limit based on your requirements\u001b[39;00m\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Get the generated response from GPT-4 Turbo\u001b[39;00m\n\u001b[1;32m    220\u001b[0m analysis_result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = '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'\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Example resume and job description\n",
    "resume = \"\"\"Vamsidhar Reddy M \n",
    "\n",
    "Data Analyst \n",
    "\n",
    "London, UK IG3 8JT \n",
    "\n",
    "Phone: +44-7717749939 \n",
    "\n",
    "E-Mail: vamsidhar.muchurami@outlook.com | https://www.linkedin.com/in/vamsidhar-muchurami-013b27197/ \n",
    "\n",
    " \n",
    "\n",
    "Professional Summary \n",
    "\n",
    "Dynamic Data Analyst with a successful 5-year career in software development and data analysis. Proven expertise in employing advanced methodologies for managing large-scale databases, datasets, conducting statistical analysis, and implementing data-driven solutions. Proficient in Power BI, Tableau, Python, ETL tools, SQL, and cloud platforms such as AWS and Azure. Well-versed in programming and experienced in machine learning frameworks including TensorFlow, PyTorch, and Scikit-learn. \n",
    "\n",
    "With a master's degree in data science computational Intelligence from Coventry University, I bring a robust blend of technical acumen and leadership skills. Recognized for a consistent ability to handle large datasets and relational databases. Seeking an opportunity to leverage my analytical and problem-solving skills in a challenging and dynamic data-centric environment. \n",
    "\n",
    "TECHNICAL SKILLS \n",
    "\n",
    "Programming Languages: Python, R, PostgreSQL \n",
    "\n",
    "Analysis Tools: Power Bi, Tableau \n",
    "\n",
    "Control Version: Git, SVM \n",
    "\n",
    "Cloud: AWS, Azure \n",
    "\n",
    "Microsoft: Microsoft 365, Word, Excel, PowerPoint \n",
    "\n",
    "Machine Learning: TensorFlow, PyTorch, Scikit-learn, Keras, NumPy \n",
    "\n",
    " \n",
    "\n",
    "EDUCATION \n",
    "\n",
    "Master of Science, Data Science Computational Intelligence \n",
    "\n",
    "Coventry University \n",
    "\n",
    "Bachelor of Science, Civil Engineering \n",
    "\n",
    "Vikrama Simhapuri University \n",
    "\n",
    " \n",
    "\n",
    "WORK EXPERIENCE \n",
    "\n",
    "Hippodrome Casino \n",
    "\n",
    "London, UK  \n",
    "\n",
    "Data Analyst - 10/2022 - Present \n",
    "\n",
    "Utilizing advanced statistical methods, machine learning algorithms, and Power BI Copilot to extract deeper insights from customer data and gaming statistics. \n",
    "\n",
    "Strengthening collaboration with cross-functional teams and senior management to align data analysis with overarching business goals. Occasionally leveraging Python programming for deeper analytical insights tailored to specific departmental needs. \n",
    "\n",
    "Creating comprehensive and visually appealing dashboards and reports. Occasionally incorporating Python programming for data manipulation and advanced visualization techniques facilitated by Copilot to streamline decision-making across departments. \n",
    "\n",
    "Implementing a structured framework that includes Python programming for data processing and insights derivation from Copilot analysis to evaluate the impact of data-driven recommendations. This enables continuous assessment and iterative improvements in strategies. \n",
    "\n",
    "Playing a pivotal role in enhancing the customer experience by providing strategic insights derived from comprehensive analysis, occasionally utilizing Python programming for sophisticated analytics. Collaborating closely with marketing teams to optimize strategies using Copilot-generated insights. \n",
    "\n",
    "Remaining updated with the latest trends in data analytics, including Python programming and tools like Power BI Copilot, to adapt and embrace new technologies and methodologies. \n",
    "\n",
    "Data Analyst \n",
    "\n",
    "EverestDx, Hyderabad, India \n",
    "\n",
    "11/2019-05/2021 \n",
    "\n",
    "Achieved Pat on the Back award for automating data retrieval for AWS. \n",
    "\n",
    "Developed and created a SAAS platform for dynamic connections and routine to connect to AWS using boto3. \n",
    "\n",
    "Used IAM services to implement Continuous integration and deployment of EC2 and Lambda services. \n",
    "\n",
    "Gathered the data from client EC2 instances to optimize and store in S3 servers. \n",
    "\n",
    "Automated the data collection using Airflow DAGs to create a CRON job and monitor the data pipeline. \n",
    "\n",
    "Implemented creative graphs and visualization to display the current cost for the client. \n",
    "\n",
    "Created Restful APIs in Django and Flask frameworks. \n",
    "\n",
    "Headed a team of 10 members and interacted with business users for requirements gathering. \n",
    "\n",
    "Managed error handling, code review, and debugging of the code. \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "08/2018-10/2019 \n",
    "\n",
    "Used OpenCV and Pillow to extract text from images of scanned documents. \n",
    "\n",
    "Pre-processed images using various techniques such as binarization, and denoising to improve accuracy. \n",
    "\n",
    "Achieved a character recognition accuracy of over 95% on the test set. \n",
    "\n",
    "Extracted data from PDF mapping using the Regex concept by executing exploratory data analysis. \n",
    "\n",
    "Deployed the OCR model in a web application using Flask and integrated it for document storage and retrieval. \n",
    "\n",
    "Created an algorithm to identify different formats of date field which varied from document and created a model to deploy and use in any machine. \n",
    "\n",
    " \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "12/2017-07/2018 \n",
    "\n",
    "Worked on Modules such as CRM, Sales, Accounting, and HR. Built a new module for Visitor management. \n",
    "\n",
    "Understanding the product, user requirements, and system specifications.  \n",
    "\n",
    "Involved in requirement analysis and design & development from scratch. \n",
    "\n",
    "Oversaw day-to-day enhancement and support while managing development activities.  \n",
    "\n",
    "Wrote the business logic for various modules in the application and resolved process-related issues. \n",
    "\n",
    "Supported the design of a data architecture to meet project and organizational requirements. \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "05/2017-11/2017 \n",
    "\n",
    "Executed the migration of Odoo ERP version 9 to version 11 and required solutions for all HR processes. \n",
    "\n",
    "Performed migration of Employee Management, Project Management, Leaves, Employee timesheets, Attendance, and IT help-desk modules to Odoo version 11 to fulfill the requirements.  \n",
    "\n",
    "Developed new modules namely Employee Appraisal, Leaves Super Manager Approval, Document Management, and IT Pieces of Equipment.  \n",
    "\n",
    "Executed Database Migration from PostgreSQL v9 to v10 for all the old modules with respective employee data. \n",
    "\n",
    "Support Team \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "11/2016-02/2017 \n",
    "\n",
    "Collected and integrated data from multiple sources both internal and external. \n",
    "\n",
    "Examined the GST Functionality with the functional team and technically implemented the functional object. Implemented Functions for GSTR1, and GSTR2 for data validations. \n",
    "\n",
    "Developed a program to validate the data of GSTN Number Validation, Invoice Format, POS, and Counterparty GSTN. Created Tables for GSTR1, and GSTR2. \n",
    "\n",
    "Managed Code Review and fixed the issues. Monitored performance-tuning activities in the project. \n",
    "\n",
    "CERTIFICATIONS \n",
    "\n",
    "The Complete Python Bootcamp from Zero to Hero in Python, Udemy. \n",
    "\n",
    "Ultimate Beginners Guide to Power BI, Udemy. \n",
    "\n",
    "Big Data Analysis with Pandas DataFrame. \n",
    "\n",
    "REFERENCE \n",
    "\n",
    "*** References available upon request. *** \"\"\"\n",
    "job_description = \"\"\" \n",
    "Eames are currently working exclusively with an investment management company that offers a range of global strategies and are now on the look out for an experience Data Analyst to join the team in Edinburgh for a one year FTC.\n",
    "\n",
    "\n",
    "The successful candidate will have experience in financial sectors, ideally with a background in the Python & SQL.\n",
    "\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "\n",
    "\n",
    "Develop enterprise data & efficiency strategy.\n",
    "Ensure smooth operation of data processes.\n",
    "Code and troubleshoot on a new platform in Python.\n",
    "Establish robust data reconciliation infrastructure.\n",
    "Design, modify, and test ETL packages using SSIS and SSRS.\n",
    "Analyze data for improved verification.\n",
    "Collaborate onboarding new capabilities.\n",
    "Support team decisions and department representation.\n",
    "\n",
    "\n",
    "Requirements:\n",
    "\n",
    "\n",
    "\n",
    "Strong SQL experience\n",
    "Understanding of Python programming concepts.\n",
    "Expertise in MS BI stack, SQL querying, views, and procedures.\n",
    "Skilled in SSIS for ETL, performance tuning, and troubleshooting.\n",
    "Familiarity with source control and database administration.\n",
    "Understanding of TDD, Agile, Scrum methodologies.\n",
    "Awareness/experience with SQL unit testing frameworks.\n",
    "\"\"\"\n",
    "\n",
    "# Format prompt for GPT-4 Turbo\n",
    "prompt = f\"This is a resume analysis for a job posting. Resume: {resume} Job description: {job_description}. Check for match.\"\n",
    "\n",
    "# Using GPT-4 Turbo to analyze the resume against the job description\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",  # GPT-4 Turbo engine\n",
    "  prompt=prompt,\n",
    "  max_tokens=200  # Adjust token limit based on your requirements\n",
    ")\n",
    "\n",
    "# Get the generated response from GPT-4 Turbo\n",
    "analysis_result = response.choices[0].text.strip()\n",
    "\n",
    "# Display the analysis result\n",
    "print(analysis_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error 404: {\n    \"error\": {\n        \"message\": \"The model `davinci-codex` does not exist or you do not have access to it.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": null,\n        \"code\": \"model_not_found\"\n    }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 196\u001b[0m\n\u001b[1;32m     26\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-iHWHlIUvY00uYVK08UqaT3BlbkFJEjxixMoXFtVlgpZjQNSv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m resume_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mVamsidhar Reddy M \u001b[39m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124m*** References available upon request. *** \u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;66;03m# Replace with the actual resume text\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m result \u001b[38;5;241m=\u001b[39m check_resume(api_key, resume_text)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mcheck_resume\u001b[0;34m(api_key, resume_text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error 404: {\n    \"error\": {\n        \"message\": \"The model `davinci-codex` does not exist or you do not have access to it.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": null,\n        \"code\": \"model_not_found\"\n    }\n}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "api_key = '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'\n",
    "def check_resume(api_key, resume_text):\n",
    "    url = \"https://api.openai.com/v1/engines/davinci-codex/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": f\"Check the resume below for accuracy and completeness: {resume_text}\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"n\": 1,\n",
    "        \"stop\": None,\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"text\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "# Example usage\n",
    "api_key = \"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\"\n",
    "resume_text = \"\"\" \n",
    "Vamsidhar Reddy M \n",
    "\n",
    "Data Analyst \n",
    "\n",
    "London, UK IG3 8JT \n",
    "\n",
    "Phone: +44-7717749939 \n",
    "\n",
    "E-Mail: vamsidhar.muchurami@outlook.com | https://www.linkedin.com/in/vamsidhar-muchurami-013b27197/ \n",
    "\n",
    " \n",
    "\n",
    "Professional Summary \n",
    "\n",
    "Dynamic Data Analyst with a successful 5-year career in software development and data analysis. Proven expertise in employing advanced methodologies for managing large-scale databases, datasets, conducting statistical analysis, and implementing data-driven solutions. Proficient in Power BI, Tableau, Python, ETL tools, SQL, and cloud platforms such as AWS and Azure. Well-versed in programming and experienced in machine learning frameworks including TensorFlow, PyTorch, and Scikit-learn. \n",
    "\n",
    "With a master's degree in data science computational Intelligence from Coventry University, I bring a robust blend of technical acumen and leadership skills. Recognized for a consistent ability to handle large datasets and relational databases. Seeking an opportunity to leverage my analytical and problem-solving skills in a challenging and dynamic data-centric environment. \n",
    "\n",
    "TECHNICAL SKILLS \n",
    "\n",
    "Programming Languages: Python, R, PostgreSQL \n",
    "\n",
    "Analysis Tools: Power Bi, Tableau \n",
    "\n",
    "Control Version: Git, SVM \n",
    "\n",
    "Cloud: AWS, Azure \n",
    "\n",
    "Microsoft: Microsoft 365, Word, Excel, PowerPoint \n",
    "\n",
    "Machine Learning: TensorFlow, PyTorch, Scikit-learn, Keras, NumPy \n",
    "\n",
    " \n",
    "\n",
    "EDUCATION \n",
    "\n",
    "Master of Science, Data Science Computational Intelligence \n",
    "\n",
    "Coventry University \n",
    "\n",
    "Bachelor of Science, Civil Engineering \n",
    "\n",
    "Vikrama Simhapuri University \n",
    "\n",
    " \n",
    "\n",
    "WORK EXPERIENCE \n",
    "\n",
    "Hippodrome Casino \n",
    "\n",
    "London, UK  \n",
    "\n",
    "Data Analyst - 10/2022 - Present \n",
    "\n",
    "Utilizing advanced statistical methods, machine learning algorithms, and Power BI Copilot to extract deeper insights from customer data and gaming statistics. \n",
    "\n",
    "Strengthening collaboration with cross-functional teams and senior management to align data analysis with overarching business goals. Occasionally leveraging Python programming for deeper analytical insights tailored to specific departmental needs. \n",
    "\n",
    "Creating comprehensive and visually appealing dashboards and reports. Occasionally incorporating Python programming for data manipulation and advanced visualization techniques facilitated by Copilot to streamline decision-making across departments. \n",
    "\n",
    "Implementing a structured framework that includes Python programming for data processing and insights derivation from Copilot analysis to evaluate the impact of data-driven recommendations. This enables continuous assessment and iterative improvements in strategies. \n",
    "\n",
    "Playing a pivotal role in enhancing the customer experience by providing strategic insights derived from comprehensive analysis, occasionally utilizing Python programming for sophisticated analytics. Collaborating closely with marketing teams to optimize strategies using Copilot-generated insights. \n",
    "\n",
    "Remaining updated with the latest trends in data analytics, including Python programming and tools like Power BI Copilot, to adapt and embrace new technologies and methodologies. \n",
    "\n",
    "Data Analyst \n",
    "\n",
    "EverestDx, Hyderabad, India \n",
    "\n",
    "11/2019-05/2021 \n",
    "\n",
    "Achieved Pat on the Back award for automating data retrieval for AWS. \n",
    "\n",
    "Developed and created a SAAS platform for dynamic connections and routine to connect to AWS using boto3. \n",
    "\n",
    "Used IAM services to implement Continuous integration and deployment of EC2 and Lambda services. \n",
    "\n",
    "Gathered the data from client EC2 instances to optimize and store in S3 servers. \n",
    "\n",
    "Automated the data collection using Airflow DAGs to create a CRON job and monitor the data pipeline. \n",
    "\n",
    "Implemented creative graphs and visualization to display the current cost for the client. \n",
    "\n",
    "Created Restful APIs in Django and Flask frameworks. \n",
    "\n",
    "Headed a team of 10 members and interacted with business users for requirements gathering. \n",
    "\n",
    "Managed error handling, code review, and debugging of the code. \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "08/2018-10/2019 \n",
    "\n",
    "Used OpenCV and Pillow to extract text from images of scanned documents. \n",
    "\n",
    "Pre-processed images using various techniques such as binarization, and denoising to improve accuracy. \n",
    "\n",
    "Achieved a character recognition accuracy of over 95% on the test set. \n",
    "\n",
    "Extracted data from PDF mapping using the Regex concept by executing exploratory data analysis. \n",
    "\n",
    "Deployed the OCR model in a web application using Flask and integrated it for document storage and retrieval. \n",
    "\n",
    "Created an algorithm to identify different formats of date field which varied from document and created a model to deploy and use in any machine. \n",
    "\n",
    " \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "12/2017-07/2018 \n",
    "\n",
    "Worked on Modules such as CRM, Sales, Accounting, and HR. Built a new module for Visitor management. \n",
    "\n",
    "Understanding the product, user requirements, and system specifications.  \n",
    "\n",
    "Involved in requirement analysis and design & development from scratch. \n",
    "\n",
    "Oversaw day-to-day enhancement and support while managing development activities.  \n",
    "\n",
    "Wrote the business logic for various modules in the application and resolved process-related issues. \n",
    "\n",
    "Supported the design of a data architecture to meet project and organizational requirements. \n",
    "\n",
    "Python Developer \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "05/2017-11/2017 \n",
    "\n",
    "Executed the migration of Odoo ERP version 9 to version 11 and required solutions for all HR processes. \n",
    "\n",
    "Performed migration of Employee Management, Project Management, Leaves, Employee timesheets, Attendance, and IT help-desk modules to Odoo version 11 to fulfill the requirements.  \n",
    "\n",
    "Developed new modules namely Employee Appraisal, Leaves Super Manager Approval, Document Management, and IT Pieces of Equipment.  \n",
    "\n",
    "Executed Database Migration from PostgreSQL v9 to v10 for all the old modules with respective employee data. \n",
    "\n",
    "Support Team \n",
    "\n",
    "Sailotech, Hyderabad, India \n",
    "\n",
    "11/2016-02/2017 \n",
    "\n",
    "Collected and integrated data from multiple sources both internal and external. \n",
    "\n",
    "Examined the GST Functionality with the functional team and technically implemented the functional object. Implemented Functions for GSTR1, and GSTR2 for data validations. \n",
    "\n",
    "Developed a program to validate the data of GSTN Number Validation, Invoice Format, POS, and Counterparty GSTN. Created Tables for GSTR1, and GSTR2. \n",
    "\n",
    "Managed Code Review and fixed the issues. Monitored performance-tuning activities in the project. \n",
    "\n",
    "CERTIFICATIONS \n",
    "\n",
    "The Complete Python Bootcamp from Zero to Hero in Python, Udemy. \n",
    "\n",
    "Ultimate Beginners Guide to Power BI, Udemy. \n",
    "\n",
    "Big Data Analysis with Pandas DataFrame. \n",
    "\n",
    "REFERENCE \n",
    "\n",
    "*** References available upon request. *** \n",
    "\"\"\" # Replace with the actual resume text\n",
    "result = check_resume(api_key, resume_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
